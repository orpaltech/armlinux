From a1605b1d8a0c8bf16c41d8ffad24bf017c2b2428 Mon Sep 17 00:00:00 2001
From: Sergey Suloev <ssuloev@orpaltech.com>
Date: Sat, 16 Nov 2019 21:52:22 +0300
Subject: [PATCH] spi: sun4i: Add DMA support for sun4i-SPI driver

---
 drivers/spi/spi-sun4i.c | 530 ++++++++++++++++++++++++++++++----------
 1 file changed, 399 insertions(+), 131 deletions(-)

diff --git a/drivers/spi/spi-sun4i.c b/drivers/spi/spi-sun4i.c
index cbfac65..0d4b9b7 100644
--- a/drivers/spi/spi-sun4i.c
+++ b/drivers/spi/spi-sun4i.c
@@ -10,6 +10,8 @@
 #include <linux/clk.h>
 #include <linux/delay.h>
 #include <linux/device.h>
+#include <linux/dmaengine.h>
+#include <linux/dma-mapping.h>
 #include <linux/interrupt.h>
 #include <linux/io.h>
 #include <linux/module.h>
@@ -21,7 +23,6 @@
 #define SUN4I_FIFO_DEPTH		64
 
 #define SUN4I_RXDATA_REG		0x00
-
 #define SUN4I_TXDATA_REG		0x04
 
 #define SUN4I_CTL_REG			0x08
@@ -30,6 +31,7 @@
 #define SUN4I_CTL_CPHA				BIT(2)
 #define SUN4I_CTL_CPOL				BIT(3)
 #define SUN4I_CTL_CS_ACTIVE_LOW			BIT(4)
+#define SUN4I_CTL_DMA_DEDICATED			BIT(5)
 #define SUN4I_CTL_LMTF				BIT(6)
 #define SUN4I_CTL_TF_RST			BIT(8)
 #define SUN4I_CTL_RF_RST			BIT(9)
@@ -49,6 +51,11 @@
 #define SUN4I_INT_STA_REG		0x10
 
 #define SUN4I_DMA_CTL_REG		0x14
+#define SUN4I_CTL_DMA_RF_READY			BIT(0)
+#define SUN4I_CTL_DMA_TF_HALF			BIT(9)
+#define SUN4I_CTL_DMA_TF_NOT_FULL		BIT(10)
+#define SUN4I_CTL_DMA_TF_EMP14			BIT(11)
+#define SUN4I_CTL_DMA_TF_EMP34			BIT(12)
 
 #define SUN4I_WAIT_REG			0x18
 
@@ -74,17 +81,25 @@
 #define SUN4I_FIFO_STA_TF_CNT_MASK		0x7f
 #define SUN4I_FIFO_STA_TF_CNT_BITS		16
 
+#define SUN4I_SPI_MODE_BITS		(SPI_CPOL | SPI_CPHA | SPI_CS_HIGH | SPI_LSB_FIRST)
+
+#define SUN4I_SPI_MAX_SPEED_HZ		100000000
+#define SUN4I_SPI_MIN_SPEED_HZ		3000
+
+#define SUN4I_SPI_DMA_TIMEOUT		(msecs_to_jiffies(1000))
+
 struct sun4i_spi {
-	struct spi_master	*master;
 	void __iomem		*base_addr;
 	struct clk		*hclk;
 	struct clk		*mclk;
 
-	struct completion	done;
-
 	const u8		*tx_buf;
 	u8			*rx_buf;
 	int			len;
+
+	bool			dma_pending;
+	struct completion	rx_dma_complete;
+	struct completion	tx_dma_complete;
 };
 
 static inline u32 sun4i_spi_read(struct sun4i_spi *sspi, u32 reg)
@@ -106,22 +121,6 @@ static inline u32 sun4i_spi_get_tx_fifo_count(struct sun4i_spi *sspi)
 	return reg & SUN4I_FIFO_STA_TF_CNT_MASK;
 }
 
-static inline void sun4i_spi_enable_interrupt(struct sun4i_spi *sspi, u32 mask)
-{
-	u32 reg = sun4i_spi_read(sspi, SUN4I_INT_CTL_REG);
-
-	reg |= mask;
-	sun4i_spi_write(sspi, SUN4I_INT_CTL_REG, reg);
-}
-
-static inline void sun4i_spi_disable_interrupt(struct sun4i_spi *sspi, u32 mask)
-{
-	u32 reg = sun4i_spi_read(sspi, SUN4I_INT_CTL_REG);
-
-	reg &= ~mask;
-	sun4i_spi_write(sspi, SUN4I_INT_CTL_REG, reg);
-}
-
 static inline void sun4i_spi_drain_fifo(struct sun4i_spi *sspi, int len)
 {
 	u32 reg, cnt;
@@ -159,6 +158,53 @@ static inline void sun4i_spi_fill_fifo(struct sun4i_spi *sspi, int len)
 	}
 }
 
+static inline void sun4i_spi_reset_fifos(struct sun4i_spi *sspi)
+{
+	u32 reg = sun4i_spi_read(sspi, SUN4I_CTL_REG);
+
+	/* reset FIFOs */
+	sun4i_spi_write(sspi, SUN4I_CTL_REG,
+			reg | SUN4I_CTL_RF_RST | SUN4I_CTL_TF_RST);
+}
+
+static void sun4i_spi_reset_hw(struct sun4i_spi *sspi)
+{
+	u32 reg;
+
+	/* clear pending interrupts */
+	sun4i_spi_write(sspi, SUN4I_INT_STA_REG, ~0);
+
+	/* disable interrupts */
+	sun4i_spi_write(sspi, SUN4I_INT_CTL_REG, 0);
+
+	sun4i_spi_reset_fifos(sspi);
+
+	/* disable DMA requests */
+	sun4i_spi_write(sspi, SUN4I_DMA_CTL_REG, 0);
+
+	/* clear dedicated DMA flag */
+	reg = sun4i_spi_read(sspi, SUN4I_CTL_REG);
+	reg &= ~SUN4I_CTL_DMA_DEDICATED;
+	sun4i_spi_write(sspi, SUN4I_CTL_REG, reg);
+}
+
+static bool sun4i_spi_can_dma(struct spi_master *master,
+			      struct spi_device *spi,
+			      struct spi_transfer *tfr)
+{
+	return tfr->len > SUN4I_FIFO_DEPTH;
+}
+
+static size_t sun4i_spi_max_transfer_size(struct spi_device *spi)
+{
+	struct spi_master *master = spi->master;
+
+	if (master->can_dma)
+		return SUN4I_MAX_XFER_SIZE;
+
+	return SUN4I_FIFO_DEPTH;
+}
+
 static void sun4i_spi_set_cs(struct spi_device *spi, bool enable)
 {
 	struct sun4i_spi *sspi = spi_master_get_devdata(spi->master);
@@ -196,44 +242,15 @@ static void sun4i_spi_set_cs(struct spi_device *spi, bool enable)
 	sun4i_spi_write(sspi, SUN4I_CTL_REG, reg);
 }
 
-static size_t sun4i_spi_max_transfer_size(struct spi_device *spi)
-{
-	return SUN4I_FIFO_DEPTH - 1;
-}
-
-static int sun4i_spi_transfer_one(struct spi_master *master,
-				  struct spi_device *spi,
-				  struct spi_transfer *tfr)
+static int sun4i_spi_prepare_message(struct spi_master *master,
+				     struct spi_message *msg)
 {
+	struct spi_device *spi = msg->spi;
 	struct sun4i_spi *sspi = spi_master_get_devdata(master);
-	unsigned int mclk_rate, div, timeout;
-	unsigned int start, end, tx_time;
-	unsigned int tx_len = 0;
-	int ret = 0;
 	u32 reg;
 
-	/* We don't support transfer larger than the FIFO */
-	if (tfr->len > SUN4I_MAX_XFER_SIZE)
-		return -EMSGSIZE;
-
-	if (tfr->tx_buf && tfr->len >= SUN4I_MAX_XFER_SIZE)
-		return -EMSGSIZE;
-
-	reinit_completion(&sspi->done);
-	sspi->tx_buf = tfr->tx_buf;
-	sspi->rx_buf = tfr->rx_buf;
-	sspi->len = tfr->len;
-
-	/* Clear pending interrupts */
-	sun4i_spi_write(sspi, SUN4I_INT_STA_REG, ~0);
-
-
 	reg = sun4i_spi_read(sspi, SUN4I_CTL_REG);
 
-	/* Reset FIFOs */
-	sun4i_spi_write(sspi, SUN4I_CTL_REG,
-			reg | SUN4I_CTL_RF_RST | SUN4I_CTL_TF_RST);
-
 	/*
 	 * Setup the transfer control register: Chip Select,
 	 * polarities, etc.
@@ -253,11 +270,218 @@ static int sun4i_spi_transfer_one(struct spi_master *master,
 	else
 		reg &= ~SUN4I_CTL_LMTF;
 
+	sun4i_spi_write(sspi, SUN4I_CTL_REG, reg);
+
+	return 0;
+}
+
+static void sun4i_spi_handle_err(struct spi_master *master,
+				 struct spi_message *msg)
+{
+	struct sun4i_spi *sspi = spi_master_get_devdata(master);
+
+	/* if we have an active DMA, then terminate */
+	if (sspi->dma_pending) {
+		dev_dbg(&master->dev, "DMA channel teardown\n");
+
+		dmaengine_terminate_sync(master->dma_tx);
+		dmaengine_terminate_sync(master->dma_rx);
+		sspi->dma_pending = 0;
+	}
+	/* and reset */
+	sun4i_spi_reset_hw(sspi);
+}
+
+static void sun4i_spi_dma_complete(void *args)
+{
+	struct completion *comp = args;
+
+	complete(comp);
+}
+
+static int sun4i_spi_dma_prep_tx(struct spi_master *master,
+				  struct spi_transfer *tfr)
+{
+	struct dma_async_tx_descriptor *chan_desc = NULL;
+	struct sun4i_spi *sspi = spi_master_get_devdata(master);
+	dma_cookie_t cookie;
+
+	reinit_completion(&sspi->tx_dma_complete);
+
+	chan_desc = dmaengine_prep_slave_sg(master->dma_tx,
+					    tfr->tx_sg.sgl, tfr->tx_sg.nents,
+					    DMA_TO_DEVICE,
+					    DMA_PREP_INTERRUPT | DMA_CTRL_ACK);
+	if (!chan_desc) {
+		dev_err(&master->dev,
+			"Couldn't prepare TX DMA slave\n");
+		return -EIO;
+	}
+
+	chan_desc->callback = sun4i_spi_dma_complete;
+	chan_desc->callback_param = &sspi->tx_dma_complete;
+
+	cookie = dmaengine_submit(chan_desc);
+	return dma_submit_error(cookie);
+}
+
+static int sun4i_spi_dma_prep_rx(struct spi_master *master,
+				  struct spi_transfer *tfr)
+{
+	struct dma_async_tx_descriptor *chan_desc = NULL;
+	struct sun4i_spi *sspi = spi_master_get_devdata(master);
+	dma_cookie_t cookie;
+
+	reinit_completion(&sspi->rx_dma_complete);
+
+	chan_desc = dmaengine_prep_slave_sg(master->dma_rx,
+					    tfr->rx_sg.sgl, tfr->rx_sg.nents,
+					    DMA_FROM_DEVICE,
+					    DMA_PREP_INTERRUPT | DMA_CTRL_ACK);
+	if (!chan_desc) {
+		dev_err(&master->dev,
+			"Couldn't prepare RX DMA slave\n");
+		return -EIO;
+	}
+
+	chan_desc->callback = sun4i_spi_dma_complete;
+	chan_desc->callback_param = &sspi->rx_dma_complete;
+
+	cookie = dmaengine_submit(chan_desc);
+	return dma_submit_error(cookie);
+}
+
+static int sun4i_spi_transfer_one_dma(struct spi_device *spi,
+				      struct spi_transfer *tfr)
+{
+	struct spi_master *master = spi->master;
+	struct sun4i_spi *sspi = spi_master_get_devdata(master);
+	long wait_status = 0;
+	int ret;
+	u32 reg = 0;
+
+	dev_dbg(&master->dev, "Using DMA mode for transfer\n");
+
+	/* setup DMA requests */
+	if (tfr->tx_buf) {
+		ret = sun4i_spi_dma_prep_tx(master, tfr);
+		if (ret)
+			goto err;
+
+		reg |= SUN4I_CTL_DMA_TF_NOT_FULL;
+	}
+
+	if (tfr->rx_buf) {
+		ret = sun4i_spi_dma_prep_rx(master, tfr);
+		if (ret)
+			goto err;
+
+		reg |= SUN4I_CTL_DMA_RF_READY;
+	}
+
+        sun4i_spi_write(sspi, SUN4I_DMA_CTL_REG, reg);
+
+	/* use dedicated DMA */
+	reg = sun4i_spi_read(sspi, SUN4I_CTL_REG);
+	reg |= SUN4I_CTL_DMA_DEDICATED;
+	sun4i_spi_write(sspi, SUN4I_CTL_REG, reg);
+
+	if (tfr->tx_buf)
+		dma_async_issue_pending(master->dma_tx);
+	if (tfr->rx_buf)
+		dma_async_issue_pending(master->dma_rx);
+
+	/* mark as DMA pending */
+	sspi->dma_pending = 1;
+
+	/* start the transfer */
+	reg = sun4i_spi_read(sspi, SUN4I_CTL_REG);
+	sun4i_spi_write(sspi, SUN4I_CTL_REG, reg | SUN4I_CTL_XCH);
+
+	if (tfr->rx_buf) {
+		/* wait only for RX callback */
+		wait_status = wait_for_completion_timeout(
+			&sspi->rx_dma_complete, SUN4I_SPI_DMA_TIMEOUT);
+
+	} else if (tfr->tx_buf) {
+		/* wait for TX callback when RX is disabled */
+                wait_status = wait_for_completion_timeout(
+                        &sspi->tx_dma_complete, SUN4I_SPI_DMA_TIMEOUT);
+	}
+
+	if (!wait_status) {
+		ret = -ETIMEDOUT;
+		goto err;
+	}
+
+	sspi->dma_pending = 0;
+	spi_finalize_current_transfer(master);
+
+	dev_dbg(&master->dev, "DMA transfer complete\n");
+
+	return 0;
+
+err:
+	dev_dbg(&master->dev, "DMA channel teardown\n");
+
+	dmaengine_terminate_sync(master->dma_tx);
+	dmaengine_terminate_sync(master->dma_rx);
+	sspi->dma_pending = 0;
+
+	sun4i_spi_reset_hw(sspi);
+
+	return ret;
+}
+
+static int sun4i_spi_transfer_one_pio(struct spi_device *spi,
+				      struct spi_transfer *tfr)
+{
+	struct spi_master *master = spi->master;
+	struct sun4i_spi *sspi = spi_master_get_devdata(master);
+	u32 reg;
+
+	/* fill the TX FIFO */
+	sun4i_spi_fill_fifo(sspi, SUN4I_FIFO_DEPTH);
+
+	/* enable the interrupts */
+	sun4i_spi_write(sspi, SUN4I_INT_CTL_REG,
+			SUN4I_INT_CTL_TC | SUN4I_INT_CTL_RF_F34);
+
+	/* start the transfer */
+	reg = sun4i_spi_read(sspi, SUN4I_CTL_REG);
+	sun4i_spi_write(sspi, SUN4I_CTL_REG, reg | SUN4I_CTL_XCH);
+
+	return 1;
+}
+
+static int sun4i_spi_transfer_one(struct spi_master *master,
+				  struct spi_device *spi,
+				  struct spi_transfer *tfr)
+{
+	struct sun4i_spi *sspi = spi_master_get_devdata(master);
+	unsigned int mclk_rate, div;
+	unsigned int tx_len = 0;
+	u32 reg;
+
+	if (!master->can_dma) {
+		/* No support for transfer larger than FIFO depth */
+		if (tfr->len > SUN4I_FIFO_DEPTH)
+			return -EINVAL;
+
+	} else if (tfr->len > SUN4I_MAX_XFER_SIZE)
+		return -EINVAL;
+
+	sspi->tx_buf = tfr->tx_buf;
+	sspi->rx_buf = tfr->rx_buf;
+	sspi->len = tfr->len;
+
+	sun4i_spi_reset_hw(sspi);
 
 	/*
 	 * If it's a TX only transfer, we don't want to fill the RX
 	 * FIFO with bogus data
 	 */
+	reg = sun4i_spi_read(sspi, SUN4I_CTL_REG);
 	if (sspi->rx_buf)
 		reg &= ~SUN4I_CTL_DHB;
 	else
@@ -307,55 +531,24 @@ static int sun4i_spi_transfer_one(struct spi_master *master,
 	sun4i_spi_write(sspi, SUN4I_BURST_CNT_REG, SUN4I_BURST_CNT(tfr->len));
 	sun4i_spi_write(sspi, SUN4I_XMIT_CNT_REG, SUN4I_XMIT_CNT(tx_len));
 
-	/*
-	 * Fill the TX FIFO
-	 * Filling the FIFO fully causes timeout for some reason
-	 * at least on spi2 on A10s
-	 */
-	sun4i_spi_fill_fifo(sspi, SUN4I_FIFO_DEPTH - 1);
-
-	/* Enable the interrupts */
-	sun4i_spi_enable_interrupt(sspi, SUN4I_INT_CTL_TC |
-					 SUN4I_INT_CTL_RF_F34);
-	/* Only enable Tx FIFO interrupt if we really need it */
-	if (tx_len > SUN4I_FIFO_DEPTH)
-		sun4i_spi_enable_interrupt(sspi, SUN4I_INT_CTL_TF_E34);
-
-	/* Start the transfer */
-	reg = sun4i_spi_read(sspi, SUN4I_CTL_REG);
-	sun4i_spi_write(sspi, SUN4I_CTL_REG, reg | SUN4I_CTL_XCH);
+	if (master->can_dma && sun4i_spi_can_dma(master, spi, tfr))
+		return sun4i_spi_transfer_one_dma(spi, tfr);
 
-	tx_time = max(tfr->len * 8 * 2 / (tfr->speed_hz / 1000), 100U);
-	start = jiffies;
-	timeout = wait_for_completion_timeout(&sspi->done,
-					      msecs_to_jiffies(tx_time));
-	end = jiffies;
-	if (!timeout) {
-		dev_warn(&master->dev,
-			 "%s: timeout transferring %u bytes@%iHz for %i(%i)ms",
-			 dev_name(&spi->dev), tfr->len, tfr->speed_hz,
-			 jiffies_to_msecs(end - start), tx_time);
-		ret = -ETIMEDOUT;
-		goto out;
-	}
-
-
-out:
-	sun4i_spi_write(sspi, SUN4I_INT_CTL_REG, 0);
-
-	return ret;
+	return sun4i_spi_transfer_one_pio(spi, tfr);
 }
 
 static irqreturn_t sun4i_spi_handler(int irq, void *dev_id)
 {
-	struct sun4i_spi *sspi = dev_id;
+	struct spi_master *master = dev_id;
+	struct sun4i_spi *sspi = spi_master_get_devdata(master);
 	u32 status = sun4i_spi_read(sspi, SUN4I_INT_STA_REG);
 
 	/* Transfer complete */
 	if (status & SUN4I_INT_CTL_TC) {
+		/* Clear the interrupt */
 		sun4i_spi_write(sspi, SUN4I_INT_STA_REG, SUN4I_INT_CTL_TC);
 		sun4i_spi_drain_fifo(sspi, SUN4I_FIFO_DEPTH);
-		complete(&sspi->done);
+		spi_finalize_current_transfer(master);
 		return IRQ_HANDLED;
 	}
 
@@ -367,21 +560,80 @@ static irqreturn_t sun4i_spi_handler(int irq, void *dev_id)
 		return IRQ_HANDLED;
 	}
 
-	/* Transmit FIFO 3/4 empty */
-	if (status & SUN4I_INT_CTL_TF_E34) {
-		sun4i_spi_fill_fifo(sspi, SUN4I_FIFO_DEPTH);
+	return IRQ_NONE;
+}
 
-		if (!sspi->len)
-			/* nothing left to transmit */
-			sun4i_spi_disable_interrupt(sspi, SUN4I_INT_CTL_TF_E34);
+static int sun4i_spi_dma_setup(struct device *dev,
+			       struct resource *res)
+{
+	struct spi_master *master = dev_get_drvdata(dev);
+	struct sun4i_spi *sspi = spi_master_get_devdata(master);
+	struct dma_slave_config dma_sconf;
+	int ret;
 
-		/* Only clear the interrupt _after_ re-seeding the FIFO */
-		sun4i_spi_write(sspi, SUN4I_INT_STA_REG, SUN4I_INT_CTL_TF_E34);
+	init_completion(&sspi->tx_dma_complete);
+	init_completion(&sspi->rx_dma_complete);
 
-		return IRQ_HANDLED;
+	master->dma_tx = dma_request_slave_channel_reason(dev, "tx");
+	if (IS_ERR(master->dma_tx)) {
+		dev_err(dev, "Unable to acquire DMA TX channel\n");
+		ret = PTR_ERR(master->dma_tx);
+		goto out;
 	}
 
-	return IRQ_NONE;
+	memset(&dma_sconf, 0, sizeof(dma_sconf));
+	dma_sconf.direction = DMA_MEM_TO_DEV;
+	dma_sconf.dst_addr_width = DMA_SLAVE_BUSWIDTH_1_BYTE;
+	dma_sconf.dst_addr = res->start + SUN4I_TXDATA_REG;
+	dma_sconf.dst_maxburst = 4;
+
+	ret = dmaengine_slave_config(master->dma_tx, &dma_sconf);
+	if (ret) {
+		dev_err(dev, "Unable to configure DMA TX slave\n");
+		goto err_rel_tx;
+	}
+
+	master->dma_rx = dma_request_slave_channel_reason(dev, "rx");
+	if (IS_ERR(master->dma_rx)) {
+		dev_err(dev, "Unable to acquire DMA RX channel\n");
+		ret = PTR_ERR(master->dma_rx);
+		goto err_rel_tx;
+	}
+
+	memset(&dma_sconf, 0, sizeof(dma_sconf));
+	dma_sconf.direction = DMA_DEV_TO_MEM;
+	dma_sconf.src_addr_width = DMA_SLAVE_BUSWIDTH_1_BYTE;
+	dma_sconf.src_addr = res->start + SUN4I_RXDATA_REG;
+	dma_sconf.src_maxburst = 4;
+
+	ret = dmaengine_slave_config(master->dma_rx, &dma_sconf);
+	if (ret) {
+		dev_err(dev, "Unable to configure DMA RX slave\n");
+		goto err_rel_rx;
+	}
+
+	/* don't set can_dma unless both channels are valid*/
+	master->can_dma = sun4i_spi_can_dma;
+
+	return 0;
+
+err_rel_rx:
+	dma_release_channel(master->dma_rx);
+err_rel_tx:
+	dma_release_channel(master->dma_tx);
+out:
+	master->dma_tx = NULL;
+	master->dma_rx = NULL;
+
+	return ret;
+}
+
+static void sun4i_spi_dma_release(struct spi_master *master)
+{
+	if (master->can_dma) {
+		dma_release_channel(master->dma_rx);
+		dma_release_channel(master->dma_tx);
+	}
 }
 
 static int sun4i_spi_runtime_resume(struct device *dev)
@@ -426,20 +678,36 @@ static int sun4i_spi_runtime_suspend(struct device *dev)
 
 static int sun4i_spi_probe(struct platform_device *pdev)
 {
+	struct device *dev = &pdev->dev;
 	struct spi_master *master;
 	struct sun4i_spi *sspi;
+	struct resource	*res;
 	int ret = 0, irq;
 
-	master = spi_alloc_master(&pdev->dev, sizeof(struct sun4i_spi));
+	master = spi_alloc_master(dev, sizeof(*sspi));
 	if (!master) {
-		dev_err(&pdev->dev, "Unable to allocate SPI Master\n");
+		dev_err(dev, "Unable to allocate SPI Master\n");
 		return -ENOMEM;
 	}
 
+	master->max_speed_hz = SUN4I_SPI_MAX_SPEED_HZ;
+	master->min_speed_hz = SUN4I_SPI_MIN_SPEED_HZ;
+	master->num_chipselect = 4;
+	master->mode_bits = SUN4I_SPI_MODE_BITS;
+	master->bits_per_word_mask = SPI_BPW_MASK(8);
+	master->set_cs = sun4i_spi_set_cs;
+	master->prepare_message = sun4i_spi_prepare_message;
+	master->transfer_one = sun4i_spi_transfer_one;
+	master->handle_err = sun4i_spi_handle_err;
+	master->max_transfer_size = sun4i_spi_max_transfer_size;
+	master->dev.of_node = dev->of_node;
+	master->auto_runtime_pm = true;
+
 	platform_set_drvdata(pdev, master);
 	sspi = spi_master_get_devdata(master);
 
-	sspi->base_addr = devm_platform_ioremap_resource(pdev, 0);
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	sspi->base_addr = devm_ioremap_resource(dev, res);
 	if (IS_ERR(sspi->base_addr)) {
 		ret = PTR_ERR(sspi->base_addr);
 		goto err_free_master;
@@ -447,79 +715,79 @@ static int sun4i_spi_probe(struct platform_device *pdev)
 
 	irq = platform_get_irq(pdev, 0);
 	if (irq < 0) {
+		dev_err(dev, "No IRQ specified\n");
 		ret = -ENXIO;
 		goto err_free_master;
 	}
 
 	ret = devm_request_irq(&pdev->dev, irq, sun4i_spi_handler,
-			       0, "sun4i-spi", sspi);
+			       0, dev_name(dev), master);
 	if (ret) {
-		dev_err(&pdev->dev, "Cannot request IRQ\n");
+		dev_err(dev, "Cannot request IRQ\n");
 		goto err_free_master;
 	}
 
-	sspi->master = master;
-	master->max_speed_hz = 100 * 1000 * 1000;
-	master->min_speed_hz = 3 * 1000;
-	master->set_cs = sun4i_spi_set_cs;
-	master->transfer_one = sun4i_spi_transfer_one;
-	master->num_chipselect = 4;
-	master->mode_bits = SPI_CPOL | SPI_CPHA | SPI_CS_HIGH | SPI_LSB_FIRST;
-	master->bits_per_word_mask = SPI_BPW_MASK(8);
-	master->dev.of_node = pdev->dev.of_node;
-	master->auto_runtime_pm = true;
-	master->max_transfer_size = sun4i_spi_max_transfer_size;
-
-	sspi->hclk = devm_clk_get(&pdev->dev, "ahb");
+	sspi->hclk = devm_clk_get(dev, "ahb");
 	if (IS_ERR(sspi->hclk)) {
-		dev_err(&pdev->dev, "Unable to acquire AHB clock\n");
+		dev_err(dev, "Unable to acquire AHB clock\n");
 		ret = PTR_ERR(sspi->hclk);
 		goto err_free_master;
 	}
 
-	sspi->mclk = devm_clk_get(&pdev->dev, "mod");
+	sspi->mclk = devm_clk_get(dev, "mod");
 	if (IS_ERR(sspi->mclk)) {
-		dev_err(&pdev->dev, "Unable to acquire module clock\n");
+		dev_err(dev, "Unable to acquire module clock\n");
 		ret = PTR_ERR(sspi->mclk);
 		goto err_free_master;
 	}
 
-	init_completion(&sspi->done);
+	ret = sun4i_spi_dma_setup(dev, res);
+	if (ret) {
+		if (ret == -EPROBE_DEFER)
+			goto err_free_master;
+
+		dev_warn(dev, "DMA transfers disabled\n");
+	}
 
 	/*
 	 * This wake-up/shutdown pattern is to be able to have the
 	 * device woken up, even if runtime_pm is disabled
 	 */
-	ret = sun4i_spi_runtime_resume(&pdev->dev);
+	ret = sun4i_spi_runtime_resume(dev);
 	if (ret) {
-		dev_err(&pdev->dev, "Couldn't resume the device\n");
+		dev_err(dev, "Couldn't resume the device\n");
 		goto err_free_master;
 	}
 
-	pm_runtime_set_active(&pdev->dev);
-	pm_runtime_enable(&pdev->dev);
-	pm_runtime_idle(&pdev->dev);
+	pm_runtime_set_active(dev);
+	pm_runtime_enable(dev);
+	pm_runtime_idle(dev);
 
-	ret = devm_spi_register_master(&pdev->dev, master);
+	ret = devm_spi_register_master(dev, master);
 	if (ret) {
-		dev_err(&pdev->dev, "cannot register SPI master\n");
+		dev_err(dev, "Couldn't register SPI master\n");
 		goto err_pm_disable;
 	}
 
 	return 0;
 
 err_pm_disable:
-	pm_runtime_disable(&pdev->dev);
-	sun4i_spi_runtime_suspend(&pdev->dev);
+	pm_runtime_disable(dev);
+	sun4i_spi_runtime_suspend(dev);
 err_free_master:
+	sun4i_spi_dma_release(master);
 	spi_master_put(master);
 	return ret;
 }
 
 static int sun4i_spi_remove(struct platform_device *pdev)
 {
+	struct spi_master *master = platform_get_drvdata(pdev);
+
 	pm_runtime_force_suspend(&pdev->dev);
 
+	sun4i_spi_dma_release(master);
+
 	return 0;
 }
 
-- 
2.17.1

